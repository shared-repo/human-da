{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\human\\.conda\\envs\\human-dl-env2\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras as tf_keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data-files/nietzsche.txt\", \"rt\") as f:\n",
    "    nietzsche_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600893\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PREFACE\\n\\n\\nSUPPOSING that Truth'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print( len(nietzsche_text) )\n",
    "nietzsche_text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'preface\\n\\n\\nsupposing that truth'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대문자 -> 소문자\n",
    "nietzsche_lower_text = nietzsche_text.lower()\n",
    "nietzsche_lower_text[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n' ' ' '!' '\"' \"'\" '(' ')' ',' '-' '.' '0' '1' '2' '3' '4' '5' '6' '7'\n",
      " '8' '9' ':' ';' '=' '?' '[' ']' '_' 'a' 'b' 'c' 'd' 'e' 'f' 'g' 'h' 'i'\n",
      " 'j' 'k' 'l' 'm' 'n' 'o' 'p' 'q' 'r' 's' 't' 'u' 'v' 'w' 'x' 'y' 'z' 'ä'\n",
      " 'æ' 'é' 'ë']\n",
      "(57,)\n"
     ]
    }
   ],
   "source": [
    "# 전체 텍스트에 포함된 문자 확인\n",
    "print( np.unique(list(nietzsche_lower_text)) )\n",
    "print( np.unique(list(nietzsche_lower_text)).shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', ' ', '!', '\"', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '=', '?', '[', ']', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'ä', 'æ', 'é', 'ë']\n",
      "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, \"'\": 4, '(': 5, ')': 6, ',': 7, '-': 8, '.': 9, '0': 10, '1': 11, '2': 12, '3': 13, '4': 14, '5': 15, '6': 16, '7': 17, '8': 18, '9': 19, ':': 20, ';': 21, '=': 22, '?': 23, '[': 24, ']': 25, '_': 26, 'a': 27, 'b': 28, 'c': 29, 'd': 30, 'e': 31, 'f': 32, 'g': 33, 'h': 34, 'i': 35, 'j': 36, 'k': 37, 'l': 38, 'm': 39, 'n': 40, 'o': 41, 'p': 42, 'q': 43, 'r': 44, 's': 45, 't': 46, 'u': 47, 'v': 48, 'w': 49, 'x': 50, 'y': 51, 'z': 52, 'ä': 53, 'æ': 54, 'é': 55, 'ë': 56}\n",
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: ':', 21: ';', 22: '=', 23: '?', 24: '[', 25: ']', 26: '_', 27: 'a', 28: 'b', 29: 'c', 30: 'd', 31: 'e', 32: 'f', 33: 'g', 34: 'h', 35: 'i', 36: 'j', 37: 'k', 38: 'l', 39: 'm', 40: 'n', 41: 'o', 42: 'p', 43: 'q', 44: 'r', 45: 's', 46: 't', 47: 'u', 48: 'v', 49: 'w', 50: 'x', 51: 'y', 52: 'z', 53: 'ä', 54: 'æ', 55: 'é', 56: 'ë'}\n"
     ]
    }
   ],
   "source": [
    "# 문자 사전 만들기\n",
    "\n",
    "set(nietzsche_lower_text) # set : 중복되지 않는 리스트\n",
    "sorted_chars = sorted( set(nietzsche_lower_text) )\n",
    "print( sorted_chars )\n",
    "\n",
    "char_to_idx = { ch:i for i, ch in enumerate(sorted_chars) } # 문자 : 숫자\n",
    "print( char_to_idx )\n",
    "\n",
    "idx_to_char = { i:ch for ch, i in char_to_idx.items() }     # 숫자 : 문자\n",
    "print( idx_to_char )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 내용\n",
    "\n",
    "# n개의 연속된 문자 -> n+1번째 문자 예측\n",
    "# (입력데이터)        (출력데이터, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 데이터 준비\n",
    "\n",
    "sequence_length = 50    # 연속된 문자 갯수\n",
    "step = 3                # stride (3문자씩 이동하면서 데이터 추출)\n",
    "\n",
    "sequences = []      # (batch크기, 입력문자갯수, 단어사전크기)\n",
    "next_chars = []     # (batch크기, 단어사전크기)\n",
    "\n",
    "for idx in range(0, len(nietzsche_lower_text) - sequence_length, step):\n",
    "    sequences.append(nietzsche_lower_text[idx:idx+sequence_length])\n",
    "    next_chars.append(nietzsche_lower_text[idx+sequence_length])\n",
    "\n",
    "# print( len(sequences), len(next_chars) )\n",
    "# print( sequences[0], next_chars[0])\n",
    "\n",
    "X = np.zeros(shape=(len(sequences), sequence_length, len(sorted_chars)))\n",
    "y = np.zeros(shape=(len(sequences), len(sorted_chars)))\n",
    "\n",
    "for si, sequence in enumerate(sequences):   # si : 입력 문장 순서 번호\n",
    "    # print(si, sequence)\n",
    "    for ci, ch in enumerate(sequence):      # ci: 한 개의 입력문장 안의 문자 순서 번호\n",
    "        X[si, ci, char_to_idx[ch]] = 1\n",
    "        y[si, char_to_idx[next_chars[si]]] = 1\n",
    "        # print(ci, ch, end=\", \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 50, 57)]          0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 128)               95232     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 57)                7353      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102585 (400.72 KB)\n",
      "Trainable params: 102585 (400.72 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 모델 구조 설계\n",
    "\n",
    "input = tf_keras.layers.Input(shape=(sequence_length, len(sorted_chars))) # (50, 57)\n",
    "x = tf_keras.layers.LSTM(units=128)(input)\n",
    "output = tf_keras.layers.Dense(units=len(sorted_chars), activation=\"softmax\")(x)\n",
    "\n",
    "model = tf_keras.models.Model(input, output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습 설계\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf_keras.optimizers.Adam(learning_rate=0.01),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1565/1565 [==============================] - 92s 57ms/step - loss: 2.0344 - accuracy: 0.4053\n",
      "Epoch 2/50\n",
      "1565/1565 [==============================] - 89s 57ms/step - loss: 1.6622 - accuracy: 0.5049\n",
      "Epoch 3/50\n",
      "1565/1565 [==============================] - 81s 52ms/step - loss: 1.5565 - accuracy: 0.5316\n",
      "Epoch 4/50\n",
      "1565/1565 [==============================] - 81s 52ms/step - loss: 1.4994 - accuracy: 0.5457\n",
      "Epoch 5/50\n",
      "1565/1565 [==============================] - 83s 53ms/step - loss: 1.4640 - accuracy: 0.5548\n",
      "Epoch 6/50\n",
      "1565/1565 [==============================] - 81s 52ms/step - loss: 1.4383 - accuracy: 0.5617\n",
      "Epoch 7/50\n",
      "1565/1565 [==============================] - 88s 56ms/step - loss: 1.4175 - accuracy: 0.5654\n",
      "Epoch 8/50\n",
      "1565/1565 [==============================] - 80s 51ms/step - loss: 1.4029 - accuracy: 0.5707\n",
      "Epoch 9/50\n",
      "1565/1565 [==============================] - 78s 50ms/step - loss: 1.3906 - accuracy: 0.5737\n",
      "Epoch 10/50\n",
      "1565/1565 [==============================] - 78s 50ms/step - loss: 1.3817 - accuracy: 0.5750\n",
      "Epoch 11/50\n",
      "1565/1565 [==============================] - 78s 50ms/step - loss: 1.3725 - accuracy: 0.5786\n",
      "Epoch 12/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3653 - accuracy: 0.5797\n",
      "Epoch 13/50\n",
      "1565/1565 [==============================] - 80s 51ms/step - loss: 1.3584 - accuracy: 0.5806\n",
      "Epoch 14/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3545 - accuracy: 0.5821\n",
      "Epoch 15/50\n",
      "1565/1565 [==============================] - 78s 50ms/step - loss: 1.3490 - accuracy: 0.5837\n",
      "Epoch 16/50\n",
      "1565/1565 [==============================] - 79s 51ms/step - loss: 1.3443 - accuracy: 0.5847\n",
      "Epoch 17/50\n",
      "1565/1565 [==============================] - 76s 48ms/step - loss: 1.3439 - accuracy: 0.5857\n",
      "Epoch 18/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3406 - accuracy: 0.5850\n",
      "Epoch 19/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.3358 - accuracy: 0.5873\n",
      "Epoch 20/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3310 - accuracy: 0.5871\n",
      "Epoch 21/50\n",
      "1565/1565 [==============================] - 79s 50ms/step - loss: 1.3320 - accuracy: 0.5872\n",
      "Epoch 22/50\n",
      "1565/1565 [==============================] - 79s 50ms/step - loss: 1.3311 - accuracy: 0.5871\n",
      "Epoch 23/50\n",
      "1565/1565 [==============================] - 83s 53ms/step - loss: 1.3272 - accuracy: 0.5888\n",
      "Epoch 24/50\n",
      "1565/1565 [==============================] - 78s 50ms/step - loss: 1.3240 - accuracy: 0.5894\n",
      "Epoch 25/50\n",
      "1565/1565 [==============================] - 78s 50ms/step - loss: 1.3246 - accuracy: 0.5888\n",
      "Epoch 26/50\n",
      "1565/1565 [==============================] - 82s 52ms/step - loss: 1.3229 - accuracy: 0.5900\n",
      "Epoch 27/50\n",
      "1565/1565 [==============================] - 81s 52ms/step - loss: 1.3240 - accuracy: 0.5899\n",
      "Epoch 28/50\n",
      "1565/1565 [==============================] - 85s 54ms/step - loss: 1.3189 - accuracy: 0.5906\n",
      "Epoch 29/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.3183 - accuracy: 0.5910\n",
      "Epoch 30/50\n",
      "1565/1565 [==============================] - 79s 50ms/step - loss: 1.3176 - accuracy: 0.5911\n",
      "Epoch 31/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3164 - accuracy: 0.5914\n",
      "Epoch 32/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.3155 - accuracy: 0.5911\n",
      "Epoch 33/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.3177 - accuracy: 0.5917\n",
      "Epoch 34/50\n",
      "1565/1565 [==============================] - 76s 48ms/step - loss: 1.3145 - accuracy: 0.5918\n",
      "Epoch 35/50\n",
      "1565/1565 [==============================] - 74s 47ms/step - loss: 1.3113 - accuracy: 0.5925\n",
      "Epoch 36/50\n",
      "1565/1565 [==============================] - 76s 49ms/step - loss: 1.3161 - accuracy: 0.5916\n",
      "Epoch 37/50\n",
      "1565/1565 [==============================] - 76s 48ms/step - loss: 1.3117 - accuracy: 0.5927\n",
      "Epoch 38/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3126 - accuracy: 0.5920\n",
      "Epoch 39/50\n",
      "1565/1565 [==============================] - 76s 49ms/step - loss: 1.3149 - accuracy: 0.5916\n",
      "Epoch 40/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3082 - accuracy: 0.5938\n",
      "Epoch 41/50\n",
      "1565/1565 [==============================] - 78s 50ms/step - loss: 1.3157 - accuracy: 0.5908\n",
      "Epoch 42/50\n",
      "1565/1565 [==============================] - 76s 49ms/step - loss: 1.3063 - accuracy: 0.5941\n",
      "Epoch 43/50\n",
      "1565/1565 [==============================] - 76s 49ms/step - loss: 1.3117 - accuracy: 0.5923\n",
      "Epoch 44/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3103 - accuracy: 0.5921\n",
      "Epoch 45/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3217 - accuracy: 0.5889\n",
      "Epoch 46/50\n",
      "1565/1565 [==============================] - 77s 49ms/step - loss: 1.3094 - accuracy: 0.5929\n",
      "Epoch 47/50\n",
      "1565/1565 [==============================] - 79s 50ms/step - loss: 1.3054 - accuracy: 0.5942\n",
      "Epoch 48/50\n",
      "1565/1565 [==============================] - 75s 48ms/step - loss: 1.3089 - accuracy: 0.5928\n",
      "Epoch 49/50\n",
      "1565/1565 [==============================] - 74s 48ms/step - loss: 1.3130 - accuracy: 0.5925\n",
      "Epoch 50/50\n",
      "1565/1565 [==============================] - 74s 47ms/step - loss: 1.3101 - accuracy: 0.5923\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X, y, batch_size=128, epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/generation-model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model = tf_keras.models.load_model('models/generation-model.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_character(preds, temperature=1.0): # temperature 값이 작을 수록 낮은 확률의 값이 선택 가능성이 낮짐\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)    \n",
    "    probas = np.random.multinomial(1, preds, 1) # 주어진 확률에 따라 다음 값 랜덤 선택\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temperature : 0.5\n",
      "s no demonstrable supremacy and a struggle leads by the"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\human\\AppData\\Local\\Temp\\ipykernel_16612\\47405372.py:3: RuntimeWarning: divide by zero encountered in log\n",
      "  preds = np.log(preds) / temperature\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " european and more protect of secret in the struggnes of\n",
      "the most believed to be always partica\n",
      "\n",
      "\n",
      "==================================================\n",
      "temperature : 1.0\n",
      "truggnes of\n",
      "the most believed to be always partical or capaburd beger attains dinglers of the unchumen foreve_, and therefore-anith, dresent which a g\n",
      "\n",
      "\n",
      "==================================================\n",
      "temperature : 1.5\n",
      "en foreve_, and therefore-anith, dresent which a good-self homeci.\" wout\n",
      "most a       it been pein too, justive is crune? and late!\n",
      "\n",
      "128\n",
      "\n",
      "=the\n",
      "trocabl\n",
      "\n",
      "\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "start_idx = np.random.randint(0, len(nietzsche_lower_text) - sequence_length)\n",
    "seed_text = nietzsche_lower_text[start_idx:start_idx + sequence_length]\n",
    "\n",
    "for temperature in [0.5, 1.0, 1.5]:\n",
    "\n",
    "    full_text = seed_text\n",
    "\n",
    "    print(\"temperature : {0}\".format(temperature))\n",
    "    print(seed_text, end=\"\")\n",
    "\n",
    "    for idx in range(100):\n",
    "        sample = np.zeros(shape=(1, sequence_length, len(sorted_chars))) # 1, 50, 57\n",
    "        for ci, c in enumerate(seed_text):\n",
    "            sample[0, ci, char_to_idx[c]] = 1\n",
    "\n",
    "        # predicted_values = model.predict(sample, verbose=0)\n",
    "        predicted_values = loaded_model.predict(sample, verbose=0)\n",
    "        \n",
    "        # selected_char_idx = predicted_values[0].argmax()\n",
    "        selected_char_idx = select_character(predicted_values[0], temperature)\n",
    "        \n",
    "        full_text += sorted_chars[selected_char_idx]\n",
    "        seed_text = full_text[idx+1:]\n",
    "\n",
    "        print(sorted_chars[selected_char_idx], end=\"\")\n",
    "\n",
    "    print(\"\\n\\n\")\n",
    "    print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-dl-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
