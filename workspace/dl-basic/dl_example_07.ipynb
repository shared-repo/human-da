{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "un9KUu4SgFkV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras as tf_keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터 준비\n",
        "(X_train, y_train), (X_test, y_test) = tf_keras.datasets.imdb.load_data(num_words=10000) # 10000 개의 단어 집합 사용"
      ],
      "metadata": {
        "id": "TX4mfH5UgouP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee69768c-fd0b-414c-fe06-3fe553dcc75c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(type(X_train))\n",
        "print(X_train.shape) # (25000, ) : 25000개의 문장 의미\n",
        "print(type(X_train[0])) # 각 문장은 숫자(단어 번호)의 리스트\n",
        "print(X_train[0])\n",
        "print(X_train[1])\n",
        "print(len(X_train[0]), len(X_train[1])) # 각 문장은 서로 다른 크기의 단어 집합 (one-hot-encoding 이 아님)\n",
        "print(np.unique(y_train, return_counts=True)) # 긍정 vs 부정"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8wwA25wg2_I",
        "outputId": "28bba17e-5297-47da-da6e-e589a6488be3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'numpy.ndarray'>\n",
            "(25000,)\n",
            "<class 'list'>\n",
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n",
            "[1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 2, 4, 1153, 9, 194, 775, 7, 8255, 2, 349, 2637, 148, 605, 2, 8003, 15, 123, 125, 68, 2, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 2, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 2, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 2, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]\n",
            "218 189\n",
            "(array([0, 1]), array([12500, 12500]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 사전 확인\n",
        "\n",
        "word_to_index = tf_keras.datasets.imdb.get_word_index()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iTCKhGZziolI",
        "outputId": "c2f4e401-35ac-4d35-e4d9-052d5e7d2491"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb_word_index.json\n",
            "\u001b[1m1641221/1641221\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( type( word_to_index ) )\n",
        "print( list( word_to_index.keys() )[:10] )\n",
        "print( list( word_to_index.values() )[:10] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8OEMDwiizgP",
        "outputId": "36300126-5adf-4891-dae4-761649c73188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "['fawn', 'tsukino', 'nunnery', 'sonja', 'vani', 'woods', 'spiders', 'hanging', 'woody', 'trawling']\n",
            "[34701, 52006, 52007, 16816, 63951, 1408, 16115, 2345, 2289, 52008]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 단어 번호로 단어를 찾기 위한 맵 만들기\n",
        "index_to_word = { value: key for key, value in word_to_index.items() }\n",
        "\n",
        "print( type( index_to_word ) )\n",
        "print( list( index_to_word.keys() )[:10] )\n",
        "print( list( index_to_word.values() )[:10] )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qhkl4rR2jXiI",
        "outputId": "2a076216-5147-4e29-d9aa-19b5b1b98ace"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "[34701, 52006, 52007, 16816, 63951, 1408, 16115, 2345, 2289, 52008]\n",
            "['fawn', 'tsukino', 'nunnery', 'sonja', 'vani', 'woods', 'spiders', 'hanging', 'woody', 'trawling']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( len(index_to_word.keys()) )\n",
        "print( index_to_word[88583] )\n",
        "# print( index_to_word[88585] ) # 없는 키의 값을 요청하면 오류\n",
        "print( index_to_word.get(88585) ) # get을 사용하면 없는 키의 값을 요청했을 때 None 반환\n",
        "print( index_to_word.get(88585, '?') ) # get을 사용하면 없는 키의 값을 요청했을 때 '?' 반환"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwIzmokWkDF3",
        "outputId": "1b5b3830-64fd-40a0-aed2-311e84c8869e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "88584\n",
            "voorhees'\n",
            "None\n",
            "?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print( X_train[0][:10] ) # 첫 번째 문장의 앞 10개의 단어(번호) 뽑기\n",
        "print( [ index_to_word.get(i, '?') for i in X_train[0][:10] ] ) # 단어사전에서 번호에 해당하는 단어 뽑기\n",
        "print( [ index_to_word.get(i-3, '?') for i in X_train[0][:10] ] ) # 단어사전에서 번호에 해당하는 단어 뽑기"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OPipF7KZk6NY",
        "outputId": "4ffbd612-f2f0-4701-a440-9f68459f0524"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65]\n",
            "['the', 'as', 'you', 'with', 'out', 'themselves', 'powerful', 'lets', 'loves', 'their']\n",
            "['?', 'this', 'film', 'was', 'just', 'brilliant', 'casting', 'location', 'scenery', 'story']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 각 문장을 BOW 형식으로 변환 -> (1, 10000)\n",
        "def vectorize_sentences(sentences, dimension=10000, bow=True): # dimension : column, 전체단어갯수, sentences : 행, 문장들\n",
        "    results = np.zeros((len(sentences), dimension))\n",
        "\n",
        "    for i, sentence in enumerate(sentences):\n",
        "        for word in sentence:\n",
        "            if bow:\n",
        "              results[i, word] += 1.  # 단어 위치에 발생 빈도 encoding\n",
        "            else:\n",
        "              results[i, word] = 1.   # 단어 위치에 1 encoding\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "4DZBFpAU_STD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 입력 데이터 변환\n",
        "X_train2 = vectorize_sentences(X_train, bow=False)\n",
        "X_test2 = vectorize_sentences(X_test, bow=False)"
      ],
      "metadata": {
        "id": "vY2wdCXgBjqM"
      },
      "execution_count": 16,
      "outputs": []
    }
  ]
}