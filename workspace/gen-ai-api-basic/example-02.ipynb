{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = \"\"\n",
    "\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY) # openai api service에 연결된 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"A transformer is a deep learning architecture that was developed by researchers at Google and is based on the multi-head attention mechanism, which was proposed in the 2017 paper \"Attention Is All You Need\".[1] Text is converted to numerical representations called tokens, and each token is converted into a vector via lookup from a word embedding table.[1] At each layer, each token is then contextualized within the scope of the context window with other (unmasked) tokens via a parallel multi-head attention mechanism, allowing the signal for key tokens to be amplified and less important tokens to be diminished.\n",
    "Transformers have the advantage of having no recurrent units, therefore requiring less training time than earlier recurrent neural architectures (RNNs) such as long short-term memory (LSTM).[2] Later variations have been widely adopted for training large language models (LLM) on large (language) datasets, such as the Wikipedia corpus and Common Crawl.[3]\n",
    "Transformers were first developed as an improvement over previous architectures for machine translation,[4][5] but have found many applications since. They are used in large-scale natural language processing, computer vision (vision transformers), reinforcement learning,[6][7] audio,[8] multimodal learning, robotics,[9] and even playing chess.[10] It has also led to the development of pre-trained systems, such as generative pre-trained transformers (GPTs)[11] and BERT[12] (bidirectional encoder representations from transformers).\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\n",
    "    **Instructions** :\n",
    "    - You are an expert assistant that summarizes text into **Korean language**.\n",
    "    - Your task is to summarize the **text** sentences in **Korean language**.\n",
    "    - Your summaries should include the following :\n",
    "        - Omit duplicate content, but increase the summary weight of duplicate content.\n",
    "        - Summarize by emphasizing concepts and arguments rather than case evidence.\n",
    "        - Summarize in 1 line.\n",
    "        - Use the format of a bullet point.\n",
    "    -text : {text}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "        { \"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 트랜스포머는 2017 논문 \"Attention Is All You Need\"에서 제안된 멀티헤드 어텐션 메커니즘을 기반으로 하는 구글 연구자들에 의해 개발된 딥러닝 아키텍처이다. 이는 순환 유닛이 없어 이전 순환 신경 아키텍처인 LSTM보다 더 적은 훈련 시간을 필요로 한다. 이후 변형들은 대규모 언어 모델을 학습시키기 위해 널리 사용되며, 기계 번역을 위해 이전 아키텍처의 개선으로 처음 개발되었지만, 현재는 다양한 분야에서 사용되고 있다.\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "human-dl-env2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
